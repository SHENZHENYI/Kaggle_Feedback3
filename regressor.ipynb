{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('pip install iterative-stratification==0.1.7')\n",
    "os.system('pip install transformers')\n",
    "os.system('pip install sentencepiece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch import nn\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "from configs.regressor0_cfg import SimpleRegressorConfig\n",
    "from src.models.regressors import BaselineRegressor\n",
    "from src.models.layers import MeanPooling\n",
    "\n",
    "from src.utils.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRegressorConfig(seed=1997, n_fold=5, target_columns=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], data_dir='./data/fb3', save_dir='./outputs', load_dir='./outputs', model='microsoft/deberta-v3-base', criterion='l2', plm_size=768, gradient_checkpointing=False, apex=True, num_workers=4, epoch=10, batch_size=8, max_len=512, encoder_lr=1e-05, decoder_lr=1e-05, weight_decay=0.01, eps=1e-06, betas=(0.9, 0.999), scheduler='cosine', warmup_ratio=0.1, num_cycles=5, print_freq=20, device='cpu', max_grad_norm=1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the config\n",
    "cfg = SimpleRegressorConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRegressorConfig(seed=42, n_fold=4, target_columns=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], data_dir='./data/fb3', save_dir='./outputs', load_dir='./outputs', model='microsoft/deberta-v3-base', criterion='l1', plm_size=768, gradient_checkpointing=True, apex=True, num_workers=4, epoch=5, batch_size=8, max_len=768, encoder_lr=2e-05, decoder_lr=2e-05, weight_decay=0.01, eps=1e-06, betas=(0.9, 0.999), scheduler='cosine', warmup_ratio=0.0, num_cycles=1.0, print_freq=20, device='cpu', max_grad_norm=1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change cfg \n",
    "cfg.device = 'cuda'\n",
    "cfg.data_dir = './data/fb3'\n",
    "cfg.save_dir = './outputs'\n",
    "cfg.max_len = 768\n",
    "cfg.gradient_checkpointing = True\n",
    "cfg.epoch = 5\n",
    "cfg.num_cycles = 1.0\n",
    "cfg.encoder_lr = 2e-5\n",
    "cfg.decoder_lr = 5e-5\n",
    "cfg.warmup_ratio = 0.0\n",
    "cfg.criterion = 'l1'\n",
    "cfg.seed = 42\n",
    "cfg.n_fold = 4\n",
    "\n",
    "cfg.layerwise_lr = 2e-5\n",
    "cfg.layerwise_lr_decay = 0.9\n",
    "cfg.layerwise_weight_decay = 0.01\n",
    "cfg.layerwise_adam_epsilon = 1e-6\n",
    "cfg.layerwise_use_bertadam = False\n",
    "\n",
    "cfg.reinit = True\n",
    "cfg.reinit_n = 1\n",
    "\n",
    "cfg.fgm = False\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  fold  \n",
       "0     3.5         3.0          3.0      4.0          3.0     2  \n",
       "1     2.5         3.0          2.0      2.0          2.5     0  \n",
       "2     3.5         3.0          3.0      3.0          2.5     1  \n",
       "3     4.5         4.5          4.5      4.0          5.0     3  \n",
       "4     3.0         3.0          3.0      2.5          2.5     3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data\n",
    "train_df = pd.read_csv(os.path.join(cfg.data_dir, 'train.csv'))\n",
    "Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train_df, train_df[cfg.target_columns])):\n",
    "    train_df.loc[val_index, 'fold'] = int(n)\n",
    "train_df['fold'] = train_df['fold'].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, ft_all_layers):\n",
    "        all_layer_embedding = torch.stack(ft_all_layers)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention layer\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "        nn.Linear(in_dim, in_dim),\n",
    "        nn.LayerNorm(in_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(in_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRegressor_LayerwiseLRD_WeightedLayer(BaselineRegressor):\n",
    "    def __init__(\n",
    "        self, \n",
    "        cfg,\n",
    "        cfg_path: str = None,\n",
    "        use_pretrained: bool = None\n",
    "    ):\n",
    "        BaselineRegressor.__init__(self, cfg, cfg_path, use_pretrained)\n",
    "        self.weighted_pool = WeightedLayerPooling(self.model_config.num_hidden_layers, 4, None)\n",
    "        #self._init_weights(self.fc)\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        last_hidden_state = self.transformer(input_ids=inputs, attention_mask=attention_mask)\n",
    "        features = self.weighted_pool(last_hidden_state[1])\n",
    "        features = self.pool(features, attention_mask)\n",
    "        out = self.fc(features)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def get_optimizer(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        # initialize lr for task specific layer\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in self.named_parameters() if \"transformer\" not in n],\n",
    "                                            \"weight_decay\": 0.0,\n",
    "                                            \"lr\": self.cfg.decoder_lr,\n",
    "                                        },]\n",
    "        # initialize lr for extra params in encoder\n",
    "        extra_params = [(n,p) for n, p in self.named_parameters() if \"transformer\" in n and \"transformer.embeddings\" not in n and \"transformer.encoder.layer\" not in n]\n",
    "        optimizer_grouped_parameters += [{\"params\": [p for n, p in extra_params if not any(nd in n for nd in no_decay)],\n",
    "                                            \"weight_decay\": self.cfg.layerwise_weight_decay,\n",
    "                                            \"lr\": self.cfg.layerwise_lr,\n",
    "                                        },]\n",
    "        optimizer_grouped_parameters += [{\"params\": [p for n, p in extra_params if any(nd in n for nd in no_decay)],\"weight_decay\": 0.0,\n",
    "                                            \"lr\": self.cfg.layerwise_lr,\n",
    "                                        },]\n",
    "        # initialize lrs for every layer\n",
    "        layers = [self.transformer.embeddings] + list(self.transformer.encoder.layer)\n",
    "        layers.reverse()\n",
    "        lr = self.cfg.layerwise_lr\n",
    "        for layer in layers:\n",
    "            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                                \"weight_decay\": self.cfg.layerwise_weight_decay,\n",
    "                                                \"lr\": lr,\n",
    "                                                },\n",
    "                                                {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                                \"weight_decay\": 0.0,\n",
    "                                                \"lr\": lr,\n",
    "                                                },]\n",
    "            lr *= self.cfg.layerwise_lr_decay\n",
    "            \n",
    "        return AdamW(optimizer_grouped_parameters,\n",
    "                            lr = self.cfg.layerwise_lr,\n",
    "                            eps = self.cfg.layerwise_adam_epsilon,\n",
    "                            correct_bias = not self.cfg.layerwise_use_bertadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_initializing_layer(model, config, layer_num):\n",
    "    for module in model.transformer.encoder.layer[-layer_num:].modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    return model \n",
    "# train for folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon = 1., emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "\n",
    "from src.utils.train_utils import AverageMeter, KeepAll, to_cuda, count_parameters\n",
    "\n",
    "\n",
    "class FGM_Trainer(Trainer):\n",
    "    def __init__(self,\n",
    "        cfg: object,\n",
    "        model: nn.Module,\n",
    "        fold: int,\n",
    "        train_samples: Union[List, pd.DataFrame] = None,\n",
    "        val_samples: Union[List, pd.DataFrame] = None,\n",
    "        test_samples: Union[List, pd.DataFrame] = None,\n",
    "        device: str = 'cpu',\n",
    "        checkpoint_path: str = None,\n",
    "        ):\n",
    "        \n",
    "        Trainer.__init__(\n",
    "            self, \n",
    "            cfg,\n",
    "            model,\n",
    "            fold,\n",
    "            train_samples,\n",
    "            val_samples,\n",
    "            test_samples,\n",
    "            device,\n",
    "            checkpoint_path,\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(log_file: str):\n",
    "        logger = logging.getLogger('trainer')\n",
    "        if logger.hasHandlers():\n",
    "            logger.handlers.clear()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler1 = logging.StreamHandler()\n",
    "        handler1.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "        handler2 = logging.FileHandler(filename=f\"{log_file}\")\n",
    "        handler2.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "        logger.addHandler(handler1)\n",
    "        logger.addHandler(handler2)\n",
    "        return logger\n",
    "    \n",
    "    def _optimize(\n",
    "        self,\n",
    "        batch: Dict,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scaler: object,\n",
    "        criterion: nn.Module,\n",
    "        scheduler: Union[torch.optim.lr_scheduler._LRScheduler, List],\n",
    "        fgm: object\n",
    "    ) -> Tuple[Dict, Dict]:\n",
    "        with torch.cuda.amp.autocast(enabled=self.cfg.apex):\n",
    "            outputs_dict, losses_dict = self._model_train_step(model, batch, criterion)\n",
    "\n",
    "        scaler.scale(losses_dict[\"loss\"]).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)\n",
    "\n",
    "        if fgm is not None:\n",
    "            fgm.attack()\n",
    "            with torch.cuda.amp.autocast(enabled = self.cfg.apex):\n",
    "                outputs_dict, losses_dict = self._model_train_step(model, batch, criterion)\n",
    "                losses_dict[\"loss\"].backward()\n",
    "            fgm.restore()\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses_dict[\"amp_scaler\"] = scaler.get_scale()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        losses_dict_detached = self._detach_loss_dict(losses_dict)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        return outputs_dict, losses_dict_detached\n",
    "\n",
    "    def train_step(self, batch: Dict, fgm) -> Tuple[Dict, Dict, float]:\n",
    "        start = time.time()\n",
    "        outputs, losses = self._optimize(batch, self.model, self.optimizer, self.scaler, \\\n",
    "                                    self.criterion, self.scheduler, fgm)\n",
    "        elapsed = time.time() - start\n",
    "        # just in case\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "        self.steps_done += 1\n",
    "        return outputs, losses, elapsed\n",
    "\n",
    "    \n",
    "    def train_epoch(self) -> None:\n",
    "        self.model.train()\n",
    "        fgm = None\n",
    "        if self.cfg.fgm:\n",
    "            fgm = FGM(model)\n",
    "        train_losses_epoch = AverageMeter()\n",
    "        start = time.time()\n",
    "        for cur_step, batch in enumerate(self.train_loader):\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = to_cuda(v, self.cfg.device)\n",
    "            _, losses, elapsed = self.train_step(batch, fgm)\n",
    "\n",
    "            train_losses_epoch.update(losses['loss'])\n",
    "            if cur_step % self.cfg.print_freq == 0 or cur_step == (len(self.train_loader)-1):\n",
    "                self.logger.info(f\"Epoch: {self.epochs_done+1}[{cur_step}/{len(self.train_loader)}] Elapsed: {elapsed} Loss: {losses['loss']:.4f}\")\n",
    "\n",
    "        self.train_losses.append(train_losses_epoch.avg)\n",
    "        epoch_time = time.time() - start\n",
    "        self.logger.info(f'Epoch{self.epochs_done+1} overall info: avg_train_loss={train_losses_epoch.avg}; {epoch_time} seconds')\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for folds\n",
    "best_scores = []\n",
    "for fold_id in range(cfg.n_fold):\n",
    "    fold_train = train_df[train_df['fold'] != fold_id].reset_index(drop=True)\n",
    "    fold_valid = train_df[train_df['fold'] == fold_id].reset_index(drop=True)\n",
    "\n",
    "    model = BaselineRegressor_LayerwiseLRD_WeightedLayer(cfg, cfg_path=None, use_pretrained=True)\n",
    "    if cfg.reinit:\n",
    "        model = re_initializing_layer(model, model.model_config, cfg.reinit_n)\n",
    "        \n",
    "    trainer = FGM_Trainer(\n",
    "        cfg=cfg,\n",
    "        model=model,\n",
    "        fold=fold_id,\n",
    "        train_samples=fold_train,\n",
    "        val_samples=fold_valid,\n",
    "        test_samples=None,\n",
    "        device=cfg.device\n",
    "    )\n",
    "    trainer.logger.info(f\"========== fold: {fold_id} training ==========\")\n",
    "    trainer.fit()\n",
    "    best_scores.append(trainer.best_score)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logger.info(f\"best scores={best_scores}\")\n",
    "trainer.logger.info(f\"cv score={np.mean(best_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdbdf9ac4859dd18aa6e80881aff95515057ddbba7efa9d8607af1951f59d5bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
