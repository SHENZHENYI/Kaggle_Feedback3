{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iterative-stratification==0.1.7 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: numpy in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from iterative-stratification==0.1.7) (1.23.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from iterative-stratification==0.1.7) (1.1.1)\n",
      "Requirement already satisfied: scipy in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from iterative-stratification==0.1.7) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn->iterative-stratification==0.1.7) (2.2.0)\n",
      "Requirement already satisfied: transformers in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (4.18.0)\n",
      "Requirement already satisfied: sacremoses in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: requests in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: filelock in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/PyYAML-6.0-py3.10-macosx-11.1-arm64.egg (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: six in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: sentencepiece in /Users/zhenyishen/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (0.1.97)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip install iterative-stratification==0.1.7')\n",
    "os.system('pip install transformers')\n",
    "os.system('pip install sentencepiece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch import nn\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "from configs.regressor0_cfg import SimpleRegressorConfig\n",
    "from src.models.regressors import BaselineRegressor\n",
    "from src.models.layers import MeanPooling\n",
    "\n",
    "from src.utils.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRegressorConfig(seed=1997, n_fold=5, target_columns=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], data_dir='./data/fb3', save_dir='./outputs', load_dir='./outputs', model='microsoft/deberta-v3-base', criterion='l2', plm_size=768, gradient_checkpointing=False, apex=True, num_workers=4, epoch=10, batch_size=8, max_len=512, encoder_lr=1e-05, decoder_lr=1e-05, weight_decay=0.01, eps=1e-06, betas=(0.9, 0.999), scheduler='cosine', warmup_ratio=0.1, num_cycles=5, print_freq=20, device='cpu', max_grad_norm=1000, train_loader_apply_mask=False, mask_prob=0.8, mask_ratio=0.3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the config\n",
    "cfg = SimpleRegressorConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRegressorConfig(seed=42, n_fold=4, target_columns=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], data_dir='./data/fb3', save_dir='./outputs', load_dir='./outputs', model='microsoft/deberta-v3-base', criterion='l1', plm_size=768, gradient_checkpointing=True, apex=True, num_workers=4, epoch=5, batch_size=8, max_len=768, encoder_lr=2e-05, decoder_lr=5e-05, weight_decay=0.01, eps=1e-06, betas=(0.9, 0.999), scheduler='cosine', warmup_ratio=0.0, num_cycles=1.0, print_freq=20, device='cuda', max_grad_norm=1000, train_loader_apply_mask=False, mask_prob=0.8, mask_ratio=0.3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change cfg \n",
    "cfg.device = 'cuda'\n",
    "cfg.data_dir = './data/fb3'\n",
    "cfg.save_dir = './outputs'\n",
    "cfg.pseudo_labels_path = './data/fb3/3090_768_4attentions_for_last4hiddens_then_cat_2fcs(equal_hidden)_init_lldr.npy'\n",
    "cfg.max_len = 768\n",
    "cfg.gradient_checkpointing = True\n",
    "cfg.epoch = 5\n",
    "cfg.num_cycles = 1.0\n",
    "cfg.encoder_lr = 2e-5\n",
    "cfg.decoder_lr = 5e-5\n",
    "cfg.warmup_ratio = 0.0\n",
    "cfg.criterion = 'l1'\n",
    "cfg.seed = 42\n",
    "cfg.n_fold = 4\n",
    "\n",
    "cfg.layerwise_lr = 2e-5\n",
    "cfg.layerwise_lr_decay = 0.9\n",
    "cfg.layerwise_weight_decay = 0.01\n",
    "cfg.layerwise_adam_epsilon = 1e-6\n",
    "cfg.layerwise_use_bertadam = False\n",
    "\n",
    "cfg.reinit = True\n",
    "cfg.reinit_n = 1\n",
    "\n",
    "cfg.fgm = False\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  fold  \n",
       "0     3.5         3.0          3.0      4.0          3.0     2  \n",
       "1     2.5         3.0          2.0      2.0          2.5     0  \n",
       "2     3.5         3.0          3.0      3.0          2.5     1  \n",
       "3     4.5         4.5          4.5      4.0          5.0     3  \n",
       "4     3.0         3.0          3.0      2.5          2.5     3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data\n",
    "train_df = pd.read_csv(os.path.join(cfg.data_dir, 'train.csv'))\n",
    "Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train_df, train_df[cfg.target_columns])):\n",
    "    train_df.loc[val_index, 'fold'] = int(n)\n",
    "train_df['fold'] = train_df['fold'].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, ft_all_layers):\n",
    "        all_layer_embedding = torch.stack(ft_all_layers)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention layer\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "        nn.Linear(in_dim, in_dim),\n",
    "        nn.LayerNorm(in_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(in_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRegressor_LayerwiseLRD_WeightedLayer(BaselineRegressor):\n",
    "    def __init__(\n",
    "        self, \n",
    "        cfg,\n",
    "        cfg_path: str = None,\n",
    "        use_pretrained: bool = None\n",
    "    ):\n",
    "        BaselineRegressor.__init__(self, cfg, cfg_path, use_pretrained)\n",
    "        #self.weighted_pool = WeightedLayerPooling(self.model_config.num_hidden_layers, 4, None)\n",
    "        #self._init_weights(self.fc)\n",
    "\n",
    "    \n",
    "\n",
    "    def get_optimizer(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        # initialize lr for task specific layer\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in self.named_parameters() if \"transformer\" not in n],\n",
    "                                            \"weight_decay\": 0.0,\n",
    "                                            \"lr\": self.cfg.decoder_lr,\n",
    "                                        },]\n",
    "        # initialize lr for extra params in encoder\n",
    "        extra_params = [(n,p) for n, p in self.named_parameters() if \"transformer\" in n and \"transformer.embeddings\" not in n and \"transformer.encoder.layer\" not in n]\n",
    "        optimizer_grouped_parameters += [{\"params\": [p for n, p in extra_params if not any(nd in n for nd in no_decay)],\n",
    "                                            \"weight_decay\": self.cfg.layerwise_weight_decay,\n",
    "                                            \"lr\": self.cfg.layerwise_lr,\n",
    "                                        },]\n",
    "        optimizer_grouped_parameters += [{\"params\": [p for n, p in extra_params if any(nd in n for nd in no_decay)],\"weight_decay\": 0.0,\n",
    "                                            \"lr\": self.cfg.layerwise_lr,\n",
    "                                        },]\n",
    "        # initialize lrs for every layer\n",
    "        layers = [self.transformer.embeddings] + list(self.transformer.encoder.layer)\n",
    "        layers.reverse()\n",
    "        lr = self.cfg.layerwise_lr\n",
    "        for layer in layers:\n",
    "            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                                \"weight_decay\": self.cfg.layerwise_weight_decay,\n",
    "                                                \"lr\": lr,\n",
    "                                                },\n",
    "                                                {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                                \"weight_decay\": 0.0,\n",
    "                                                \"lr\": lr,\n",
    "                                                },]\n",
    "            lr *= self.cfg.layerwise_lr_decay\n",
    "            \n",
    "        return AdamW(optimizer_grouped_parameters,\n",
    "                            lr = self.cfg.layerwise_lr,\n",
    "                            eps = self.cfg.layerwise_adam_epsilon,\n",
    "                            correct_bias = not self.cfg.layerwise_use_bertadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_initializing_layer(model, config, layer_num):\n",
    "    for module in model.transformer.encoder.layer[-layer_num:].modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    return model \n",
    "# train for folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon = 1., emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "\n",
    "from src.utils.train_utils import AverageMeter, KeepAll, to_cuda, count_parameters\n",
    "\n",
    "\n",
    "class FGM_Trainer(Trainer):\n",
    "    def __init__(self,\n",
    "        cfg: object,\n",
    "        model: nn.Module,\n",
    "        fold: int,\n",
    "        train_samples: Union[List, pd.DataFrame] = None,\n",
    "        val_samples: Union[List, pd.DataFrame] = None,\n",
    "        test_samples: Union[List, pd.DataFrame] = None,\n",
    "        device: str = 'cpu',\n",
    "        checkpoint_path: str = None,\n",
    "        ):\n",
    "        \n",
    "        Trainer.__init__(\n",
    "            self, \n",
    "            cfg,\n",
    "            model,\n",
    "            fold,\n",
    "            train_samples,\n",
    "            val_samples,\n",
    "            test_samples,\n",
    "            device,\n",
    "            checkpoint_path,\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(log_file: str):\n",
    "        logger = logging.getLogger('trainer')\n",
    "        if logger.hasHandlers():\n",
    "            logger.handlers.clear()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler1 = logging.StreamHandler()\n",
    "        handler1.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "        handler2 = logging.FileHandler(filename=f\"{log_file}\")\n",
    "        handler2.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "        logger.addHandler(handler1)\n",
    "        logger.addHandler(handler2)\n",
    "        return logger\n",
    "    \n",
    "    def _optimize(\n",
    "        self,\n",
    "        batch: Dict,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scaler: object,\n",
    "        criterion: nn.Module,\n",
    "        scheduler: Union[torch.optim.lr_scheduler._LRScheduler, List],\n",
    "        fgm: object\n",
    "    ) -> Tuple[Dict, Dict]:\n",
    "        with torch.cuda.amp.autocast(enabled=self.cfg.apex):\n",
    "            outputs_dict, losses_dict = self._model_train_step(model, batch, criterion)\n",
    "\n",
    "        scaler.scale(losses_dict[\"loss\"]).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)\n",
    "\n",
    "        if fgm is not None:\n",
    "            fgm.attack()\n",
    "            with torch.cuda.amp.autocast(enabled = self.cfg.apex):\n",
    "                outputs_dict, losses_dict = self._model_train_step(model, batch, criterion)\n",
    "                losses_dict[\"loss\"].backward()\n",
    "            fgm.restore()\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses_dict[\"amp_scaler\"] = scaler.get_scale()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        losses_dict_detached = self._detach_loss_dict(losses_dict)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        return outputs_dict, losses_dict_detached\n",
    "\n",
    "    def train_step(self, batch: Dict, fgm) -> Tuple[Dict, Dict, float]:\n",
    "        start = time.time()\n",
    "        outputs, losses = self._optimize(batch, self.model, self.optimizer, self.scaler, \\\n",
    "                                    self.criterion, self.scheduler, fgm)\n",
    "        elapsed = time.time() - start\n",
    "        # just in case\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "        self.steps_done += 1\n",
    "        return outputs, losses, elapsed\n",
    "\n",
    "    \n",
    "    def train_epoch(self) -> None:\n",
    "        self.model.train()\n",
    "        fgm = None\n",
    "        if self.cfg.fgm:\n",
    "            fgm = FGM(model)\n",
    "        train_losses_epoch = AverageMeter()\n",
    "        start = time.time()\n",
    "        for cur_step, batch in enumerate(self.train_loader):\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = to_cuda(v, self.cfg.device)\n",
    "            _, losses, elapsed = self.train_step(batch, fgm)\n",
    "\n",
    "            train_losses_epoch.update(losses['loss'])\n",
    "            if cur_step % self.cfg.print_freq == 0 or cur_step == (len(self.train_loader)-1):\n",
    "                self.logger.info(f\"Epoch: {self.epochs_done+1}[{cur_step}/{len(self.train_loader)}] Elapsed: {elapsed} Loss: {losses['loss']:.4f}\")\n",
    "\n",
    "        self.train_losses.append(train_losses_epoch.avg)\n",
    "        epoch_time = time.time() - start\n",
    "        self.logger.info(f'Epoch{self.epochs_done+1} overall info: avg_train_loss={train_losses_epoch.avg}; {epoch_time} seconds')\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labels = np.load(cfg.pseudo_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978 978\n",
      "1955 977\n",
      "2933 978\n",
      "3911 978\n"
     ]
    }
   ],
   "source": [
    "# collect pseudo data of the according folds\n",
    "best_scores = []\n",
    "gts = []\n",
    "oofs = []\n",
    "texts = []\n",
    "prev_oof_idx = 0\n",
    "for fold_id in range(cfg.n_fold):\n",
    "    fold_train = train_df[train_df['fold'] != fold_id].reset_index(drop=True)\n",
    "    fold_valid = train_df[train_df['fold'] == fold_id].reset_index(drop=True)\n",
    "    gts.append(fold_valid[cfg.target_columns].values)\n",
    "    cur_len = len(fold_valid)\n",
    "    oofs.append(pseudo_labels[prev_oof_idx:prev_oof_idx+cur_len])\n",
    "    prev_oof_idx += cur_len\n",
    "    texts.append(fold_valid['full_text'].values)\n",
    "    print(prev_oof_idx, cur_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_folds(texts, oofs, exclud_fold: int):\n",
    "    joint_texts = []\n",
    "    oof_scores = []\n",
    "    for i in range(cfg.n_fold):\n",
    "        if i == exclud_fold:\n",
    "            continue\n",
    "        joint_texts.append(texts[i])\n",
    "        oof_scores.append(oofs[i])\n",
    "    return np.hstack(joint_texts), np.vstack(oof_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7224693 2.6182752 2.8968425 2.7458382 2.4491014 2.6532686]\n",
      "[2.5 2.5 3.  2.  2.  2.5]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           full_text  cohesion    syntax  \\\n",
      "0  Dear, Principal\\n\\nIf u change the school poli...  2.905943  3.001930   \n",
      "1  A positive attitude is the key to success for ...  3.725058  3.745834   \n",
      "2  Philosopher, physician, and humanitarian Alber...  3.708426  3.690219   \n",
      "3  10pm curfews is a bad idea, why? Because teens...  3.711316  3.701645   \n",
      "4  Although some say students will not be benefit...  2.943587  2.945189   \n",
      "\n",
      "   vocabulary  phraseology   grammar  conventions  \n",
      "0    3.062518     2.970802  2.997655     3.199598  \n",
      "1    3.884596     3.888378  3.571057     3.902278  \n",
      "2    4.027471     3.939272  3.884748     3.740093  \n",
      "3    3.786729     3.917583  3.782827     3.496711  \n",
      "4    3.103554     2.782857  2.956854     3.307884  \n"
     ]
    }
   ],
   "source": [
    "# train for folds\n",
    "best_scores = []\n",
    "for fold_id in range(cfg.n_fold):\n",
    "    fold_train = train_df[train_df['fold'] != fold_id].reset_index(drop=True)\n",
    "    fold_valid = train_df[train_df['fold'] == fold_id].reset_index(drop=True)\n",
    "\n",
    "    # load pseudo data\n",
    "    fold_train_text_pseudo, fold_train_oofs_pseudo = join_folds(texts, oofs, exclud_fold=fold_id)\n",
    "    fold_train_pseudo = pd.DataFrame({\n",
    "        'full_text': fold_train_text_pseudo,\n",
    "        'cohesion': fold_train_oofs_pseudo[:,0],\n",
    "        'syntax': fold_train_oofs_pseudo[:,1],\n",
    "        'vocabulary': fold_train_oofs_pseudo[:,2],\n",
    "        'phraseology': fold_train_oofs_pseudo[:,3],\n",
    "        'grammar': fold_train_oofs_pseudo[:,4],\n",
    "        'conventions': fold_train_oofs_pseudo[:,5],\n",
    "        })\n",
    "    \n",
    "    model = BaselineRegressor_LayerwiseLRD_WeightedLayer(cfg, cfg_path=None, use_pretrained=True)\n",
    "    if cfg.reinit:\n",
    "        model = re_initializing_layer(model, model.model_config, cfg.reinit_n)\n",
    "    \n",
    "    cfg.epoch = 5\n",
    "    pseudo_labels_trainer = FGM_Trainer(\n",
    "        cfg=cfg,\n",
    "        model=model,\n",
    "        fold=fold_id,\n",
    "        train_samples=fold_train_pseudo,\n",
    "        val_samples=fold_valid,\n",
    "        test_samples=None,\n",
    "        device=cfg.device\n",
    "    )\n",
    "    pseudo_labels_trainer.logger.info(f\"========== fold: {fold_id} training pseudo==========\")\n",
    "    pseudo_labels_trainer.fit()\n",
    "\n",
    "    '''\n",
    "    cfg.epoch = 5\n",
    "    trainer = FGM_Trainer(\n",
    "        cfg=cfg,\n",
    "        model=model,\n",
    "        fold=fold_id,\n",
    "        train_samples=fold_train,\n",
    "        val_samples=fold_valid,\n",
    "        test_samples=None,\n",
    "        device=cfg.device\n",
    "    )\n",
    "    trainer.logger.info(f\"========== fold: {fold_id} training true==========\")\n",
    "    trainer.fit()\n",
    "    '''\n",
    "    best_scores.append(pseudo_labels_trainer.best_score)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logger.info(f\"best scores={best_scores}\")\n",
    "trainer.logger.info(f\"cv score={np.mean(best_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdbdf9ac4859dd18aa6e80881aff95515057ddbba7efa9d8607af1951f59d5bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
